{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69bde890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "# Ensure NLTK can find your data by checking the stopwords and punkt resources\n",
    "try:\n",
    "    # Example: Check stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(\"--- Stopwords loaded successfully. Here is the list of stop words: ---\\n\")\n",
    "    print(stop_words)\n",
    "\n",
    "    # Example: Check punkt tokenizer\n",
    "    sample_text = \"Hello world! This is a test.\"\n",
    "    print(\"\\n--- Sample Text: ---\\n\" + sample_text)\n",
    "\n",
    "    tokenized_text = word_tokenize(sample_text)\n",
    "    print(\"\\n--- Punkt tokenizer loaded successfully. Here is the tokenized text: ---\")\n",
    "    print(tokenized_text)\n",
    "    \n",
    "    #filtered_stop_words = [word for word in stop_words if len(word) > 1]\n",
    "    #print(\"\\n--- Filtered out one-letter stopwords: \")\n",
    "    #print(filtered_stop_words)\n",
    "\n",
    "    #Example 2: Check punkt tokenizer but without one letter stopwords including a...? so maybe remove this idk\n",
    "    filtered_tokenized_text = [word for word in tokenized_text if word.lower() not in stop_words or len(word) > 1]\n",
    "    print(\"\\n--- Filtered Text (without one-letter stopwords): ---\")\n",
    "    print(filtered_tokenized_text)\n",
    "\n",
    "except LookupError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b281d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, category_files, language='english', stop_words=True):\n",
    "        self.category_files = category_files\n",
    "        self.language = language\n",
    "        self.stop_words = stop_words\n",
    "        self.stop_words_list = set()\n",
    "        self.category_data = {}\n",
    "        \n",
    "        if self.stop_words: #is stop words is true, we are using/ considering/ discluding stop words. \n",
    "            print(\"\\n------------------- Staring 3 Way Text Classification Discluding Stop Words: -----------------------\")\n",
    "            self._load_stopwords()\n",
    "        else: \n",
    "            print(\"\\n----------------------------- Starting 3 Way Text Classification : ----------------------------------\")\n",
    "\n",
    "        self._process_files() \n",
    "\n",
    "        \n",
    "    def _load_stopwords(self):\n",
    "        if self.language == 'english':\n",
    "            self.stop_words_list = set(stopwords.words('english'))\n",
    "        elif self.language == 'spanish':\n",
    "            self.stop_words_list = set(stopwords.words('spanish'))\n",
    "        else:\n",
    "            raise ValueError(\"Supported languages: 'english' or 'spanish'\")\n",
    "            \n",
    "        print(f\"\\n--- {self.language.capitalize()} stopwords loaded: ---\")\n",
    "        print(self.stop_words_list)\n",
    "        \n",
    "    def extract_words(self,text):\n",
    "        if self.stop_words: #if discluding stop words. \n",
    "            words = [\n",
    "                word.lower() for word in word_tokenize(text)\n",
    "                if any(c.isalpha() for c in word) and word.lower() not in self.stop_words_list\n",
    "            ] \n",
    "        else:      \n",
    "            words = [\n",
    "                word.lower() for word in word_tokenize(text)\n",
    "                if any(c.isalpha() for c in word)\n",
    "             ] \n",
    "        return words   \n",
    "    \n",
    "    \n",
    "    def _process_files(self):\n",
    "        # \"Items () method in the dictionary is used to return each item in a dictionary as tuples in a list\"\n",
    "        for category, filename in self.category_files.items(): #remeber category_files is a dictionary with keys: categories, val: txt file\n",
    "            words, line_count = self.load_file(filename) #load_file outputs words, a list of words from the file, and line count is num of line \n",
    "            self.category_data[category] = words #create a dictionary with categories name, and the list of words associated\n",
    "            \n",
    "            #For debugging\n",
    "            print(f\"\\n--- Full Content in '{filename}' for category '{category}' ---\")\n",
    "            print(words, line_count) \n",
    "            if self.stop_words: \n",
    "                print(f\"--- There is {len(words)} words in {filename} when REMOVING stop words. ---\") \n",
    "            else: \n",
    "                print(f\"--- There is {len(words)} words in {filename} when INCLUDING stop words. ---\")\n",
    "            \n",
    "\n",
    "    def load_file(self, file_name):\n",
    "        words = []\n",
    "        total_lines = 0\n",
    "        \n",
    "        try: \n",
    "            with open(file_name, \"r\") as f:\n",
    "                for line in f:\n",
    "                    total_lines += 1\n",
    "                    extracted = self.extract_words(line)\n",
    "                    words.extend(extracted) \n",
    "        #To debug easier :)            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {file_name} not found.\")\n",
    "\n",
    "        return words, total_lines\n",
    " \n",
    "    def calculate_word_frequency(self):\n",
    "        frequency = {} \n",
    "   \n",
    "        #for bedugging print \n",
    "        total_words = sum(len(words) for words in self.category_data.values())\n",
    "        print(f\"\\n--- Full category_data dictionary of length {total_words} ---\")\n",
    "        print(self.category_data)\n",
    "        \n",
    "        #enumerate creates a index for dictionary items  (index, (category, words))\n",
    "        for i, (category, words) in enumerate(self.category_data.items()): \n",
    "            category_index = i\n",
    "            for word in words: \n",
    "                if word in frequency.keys(): #if already in list \n",
    "                    frequency[word][category_index] += 1\n",
    "                else: \n",
    "                    freq_list = [0] * len(self.category_data)  # One slot for each category for example we have three in our examples[0,0,0]\n",
    "                    freq_list[category_index] = 1  \n",
    "                    frequency[word] = freq_list #add to the dictionary for each word, a list of frequency [cat1,cat2,cat3]\n",
    "                    \n",
    "        print(f\"\\n--- Calculated Word Frequencies: ---\\n {frequency} \")\n",
    "        return frequency\n",
    "   \n",
    "    def calculate_word_probability(self, frequency):\n",
    "        probability = {} #dictionary to store compiled datat\n",
    "\n",
    "        total_words_per_category = {category: len(words) for category, words in self.category_data.items()}\n",
    "        unique_words = len(frequency) \n",
    "\n",
    "        for word, counts in frequency.items(): #iterate over each word in dictionary \n",
    "            probabilities = [] #for each word we calculate probability in each category\n",
    "            for i, category_count in enumerate(counts): #loops through the count of freq of word in each category [cat1,cat2,cat3] creating an index for each with enumerate\n",
    "                total_words_in_category = total_words_per_category[list(self.category_data.keys())[i]] #per key-category, calculate number of words\n",
    "                prob = (category_count + 1) / (total_words_in_category + unique_words) #Laplace smoothing formula\n",
    "                probabilities.append(prob) #add probability to current category  \n",
    "            probability[word] = probabilities #creates an object like key:carbon [prob for cat1, prob for cat 2, prob for cat3] for each word but for each word, adding it to the dictionary \n",
    "        \n",
    "        print(\"\\n--- Word Probability ---\")\n",
    "        print(probability)\n",
    "        return probability\t    \n",
    "    \n",
    "    def classify(self, text, probability_categories):\n",
    "        words = self.extract_words(text) #tokenize\n",
    "        category_scores = {category: 1 for category in self.category_data} #intialize val of 1 for tuple\n",
    "\n",
    "        for word in words: #for each word in list of tokenized words \n",
    "            if word in probability_categories:  # check if exists in prob. dictionary\n",
    "                for i, category in enumerate(self.category_data.keys()): #use enum. to create index for each category in dictionary of cat\n",
    "                    category_scores[category] *= probability_categories[word][i]  \n",
    "                    #category_scores[category] this refers to the current score for the specific category\n",
    "                    #multiplies the score of current category by the prob of the current word in that category\n",
    "                    #probability_categories[word][i] is the probability of the word in the index i of probability dictionary.\n",
    "\n",
    "        total_score = sum(category_scores.values())\n",
    "        return {category: score / total_score for category, score in category_scores.items()}\n",
    "    \n",
    "    def test_classify(self,unseen_text,probability_categories): \n",
    "        print(\"\\n--- Testing Classification for Unseen Meals ---\\n\")\n",
    "        \n",
    "        for text in unseen_text:\n",
    "            result = self.classify(text, probability_categories)\n",
    "            \n",
    "            # Extract the probabilities for each category\n",
    "            category_scores = result\n",
    "            max_category = max(category_scores, key=category_scores.get)\n",
    "            max_score = category_scores[max_category]\n",
    "            \n",
    "            # If the highest probability difference between categories is greater than 0.25, classify it\n",
    "            if max_score - min(category_scores.values()) > 0.25:\n",
    "                print(f\"{text} is classified as {max_category} with a probability of {max_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"{text} is classified as neutral with probabilities:\")\n",
    "                for category, score in category_scores.items():\n",
    "                    print(f\"  {category}: {score:.4f}\")\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1074b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    unseen_texts = [\n",
    "        \"Grilled tofu with vegetables and quinoa\",\n",
    "        \"Cheese pizza with mushrooms\",\n",
    "        \"Chicken stir-fry with rice\",\n",
    "        \"Oatmeal with almond milk and berries\",\n",
    "        \"Bacon and eggs with toast\"\n",
    "    ]\n",
    "    \n",
    "    with open(\"example1_food_classification.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        sys.stdout = f \n",
    "\n",
    "        print(\"============================================================================================\")\n",
    "        print(\"PROBLEM 1: FOOD CLASSIFICATION (Vegan, Vegetarian, Omnivorous)\")\n",
    "        print(\"============================================================================================\\n\")\n",
    "        \n",
    "        print(\"Unseen texts for classification:\")\n",
    "        for i, text in enumerate(unseen_texts, 1):\n",
    "            print(f\"{i}. {text}\")\n",
    "        \n",
    "        # --- EXCLUDING STOP WORDS FIRST ---\n",
    "        print(\"\\n---PROBLEM 1, EXAMPLE 1 EXCLUDING STOP WORDS ---\\n\")\n",
    "        classifier = NaiveBayesClassifier(\n",
    "            category_files={\n",
    "                'vegan': 'data/vegan.txt',\n",
    "                'vegetarian': 'data/vegetarian.txt',\n",
    "                'omnivorous': 'data/omnivorous.txt'\n",
    "            },\n",
    "            language='english',\n",
    "            stop_words=True\n",
    "        )\n",
    "        freq = classifier.calculate_word_frequency()\n",
    "        probability_categories = classifier.calculate_word_probability(freq)\n",
    "        classifier.test_classify(unseen_texts, probability_categories)\n",
    "\n",
    "        # --- INCLUDING STOP WORDS NEXT ---\n",
    "        print(\"\\n---PROBLEM 1, EXAMPLE 2 INCLUDING STOP WORDS ---\\n\")\n",
    "        print(\"STEP 1: Initializing classifier with stop words included\")\n",
    "        classifier0 = NaiveBayesClassifier(\n",
    "            category_files={\n",
    "                'vegan': 'data/vegan.txt',\n",
    "                'vegetarian': 'data/vegetarian.txt',\n",
    "                'omnivorous': 'data/omnivorous.txt'\n",
    "            },\n",
    "            language='english',\n",
    "            stop_words=False\n",
    "        )\n",
    "        freq0 = classifier0.calculate_word_frequency()\n",
    "        probability_categories0 = classifier0.calculate_word_probability(freq0)\n",
    "        classifier0.test_classify(unseen_texts, probability_categories0)\n",
    "        print(\"\\nNOTE: In comparison to the run including stop words, we observe that removing stop words may slightly decrease classification effectiveness depending on the text.\\n\")\n",
    "\n",
    "    sys.stdout = sys.__stdout__\n",
    "    \n",
    "    unseen_texts1 = [\n",
    "        \"Por fracción molar, el aire seco contiene un 78% de nitrógen, un 20% de oxígeno, un 0.04% de dióxido de carbono y pequeñas cantidades de otros gases traza.\",\n",
    "        \"La inflación afecta el poder adquisitivo de los consumidores.\",\n",
    "        \"El cálculo diferencial estudia el cambio de funciones y sus derivadas.\",\n",
    "        \"El PIB mide el crecimiento económico de un país.\",\n",
    "        \"Las funciones trigonométricas son esenciales en la física y la ingeniería.\"\n",
    "    ]\n",
    "    \n",
    "    with open(\"example2_subject_classification.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        sys.stdout = f\n",
    "\n",
    "        print(\"============================================================================================\")\n",
    "        print(\"PROBLEM 2: SUBJECT CLASSIFICATION (Biología, Matemáticas, Economía)\")\n",
    "        print(\"============================================================================================\\n\")\n",
    "\n",
    "        print(\"Unseen texts for classification:\")\n",
    "        for i, text in enumerate(unseen_texts1, 1):\n",
    "            print(f\"{i}. {text}\")\n",
    "        \n",
    "        # --- EXCLUDING STOP WORDS FIRST ---\n",
    "        print(\"\\n--- PROBLEMA 2, EJEMPLO 1 EXCLUDING STOP WORDS ---\\n\")\n",
    "        classifier2 = NaiveBayesClassifier(\n",
    "            category_files={\n",
    "                'biologia': 'data/biologia.txt',\n",
    "                'economia': 'data/economia.txt',\n",
    "                'matematicas': 'data/matematicas.txt'\n",
    "            },\n",
    "            language='spanish',\n",
    "            stop_words=True\n",
    "        )\n",
    "        freq2 = classifier2.calculate_word_frequency()\n",
    "        probability_categories2 = classifier2.calculate_word_probability(freq2)\n",
    "        classifier2.test_classify(unseen_texts1, probability_categories2)\n",
    "\n",
    "        # --- INCLUDING STOP WORDS NEXT ---\n",
    "        print(\"\\n--- PROBLEMA 2, EJEMPLO 2 INCLUDING STOP WORDS ---\\n\")\n",
    "        classifier1 = NaiveBayesClassifier(\n",
    "            category_files={\n",
    "                'biologia': 'data/biologia.txt',\n",
    "                'economia': 'data/economia.txt',\n",
    "                'matematicas': 'data/matematicas.txt'\n",
    "            },\n",
    "            language='spanish',\n",
    "            stop_words=False\n",
    "        )\n",
    "        freq1 = classifier1.calculate_word_frequency()\n",
    "        probability_categories1 = classifier1.calculate_word_probability(freq1)\n",
    "        classifier1.test_classify(unseen_texts1, probability_categories1)\n",
    "        print(\"\\nNOTE: Compared to the run including stop words, removing stop words may slightly reduce classification accuracy depending on the text.\\n\")\n",
    "\n",
    "    sys.stdout = sys.__stdout__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
